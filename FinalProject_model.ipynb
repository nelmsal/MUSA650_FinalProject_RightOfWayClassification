{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nelmsal/MUSA650_FinalProject_RightOfWayClassification/blob/main/FinalProject_model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "23nQOAYNt_fe"
      },
      "source": [
        "# Modeling"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7pvT5rwiuoCJ"
      },
      "source": [
        "## Import Packages & Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0ZKwHthms3Js",
        "outputId": "112f43cc-6dbc-478e-d525-1f7ee7e8c905"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3kUlFy5Ht3aj",
        "outputId": "5a1288fe-a372-4e01-99e7-d54967701cf1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/Penn/MUSA-650/FinalProject\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import json\n",
        "import numpy as np\n",
        "\n",
        "PROJ_DIR = 'drive/MyDrive/Penn/MUSA-650/FinalProject'\n",
        "\n",
        "%cd {PROJ_DIR}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KDW58bu730gA"
      },
      "source": [
        "### Image, Mask, & Information Dataframes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "9DdbTWyJ2_1i"
      },
      "outputs": [],
      "source": [
        "#DATA_DIR = PROJ_DIR + '/' + 'data'\n",
        "DATA_DIR = 'data'\n",
        "\n",
        "IMAGE_PATH = DATA_DIR + '/clean/' + 'sf_naip_images.parquet'\n",
        "MASK_PATH = DATA_DIR + '/clean/' + 'sf_naip_mask.parquet'\n",
        "INFO_PATH = DATA_DIR + '/clean/' + 'sf_naip_info.parquet'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NXxTlUafbXZ9",
        "outputId": "dc609054-4ce1-445d-8d6b-34f7ccd90451"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: fsspec>=0.3.3 in /usr/local/lib/python3.7/dist-packages (2022.3.0)\n"
          ]
        }
      ],
      "source": [
        "!python -m pip install 'fsspec>=0.3.3'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "lgDq68uvgdcU"
      },
      "outputs": [],
      "source": [
        "def df_to_array(arr_df):\n",
        "  # import csv of image data \n",
        "  ## pre-flattened\n",
        "  cols = [col for col in list(arr_df) if 'Unnamed' not in col]\n",
        "  arr_df = arr_df[cols]\n",
        "\n",
        "  # list of shape index colnames\n",
        "  icols = [col for col in list(arr_df) if 'ass' not in col]\n",
        "  # make array out of rgb dataframe\n",
        "  #image_arr = np.stack(arr_df[icols].apply(list, axis=1))\n",
        "  image_arr = arr_df[icols].to_numpy(dtype='int')\n",
        "  print('Shape: {}'.format(image_arr.shape))\n",
        "\n",
        "  return image_arr\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "f1E7ee5fL-uu"
      },
      "outputs": [],
      "source": [
        "import dask.dataframe as dd\n",
        "\n",
        "img_dd = dd.read_parquet(\n",
        "             IMAGE_PATH,\n",
        "             blocksize=1000000,\n",
        "             sample=655360,\n",
        "             dtype='int'\n",
        "             )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gIREgpGwM6H7",
        "outputId": "3edf01a7-0ac6-4b6f-9f01-cf19afa6515f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape: (18811, 65536)\n"
          ]
        }
      ],
      "source": [
        "img_arr = df_to_array(img_dd.compute(scheduler='processes'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gYWNdPHf9cDh"
      },
      "outputs": [],
      "source": [
        "#mask_df = pd.read_csv(MASK_PATH)\n",
        "mask_arr = df_to_array(pd.read_parquet(MASK_PATH))\n",
        "info_df = pd.read_parquet(INFO_PATH)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XazlWhmB4OdZ"
      },
      "source": [
        "### Prepare Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "L4Sl4op5cp2s"
      },
      "outputs": [],
      "source": [
        "img_shape = (128, 128, 4)\n",
        "mask_shape = (128, 128, 1)\n",
        "rgb_shape = (128,128,3)\n",
        "\n",
        "# get shape\n",
        "def unflatten(array, new_shape):\n",
        "  # shape initial\n",
        "  n_samples = array.shape[0]\n",
        "  flat_shape = array.shape[1]\n",
        "\n",
        "  new_shape = [n_samples] + list(new_shape)\n",
        "\n",
        "  return array.reshape(new_shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nInjS-2Z5hrm"
      },
      "source": [
        "### Train / Test Split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3HI2Tmgp5fVq",
        "outputId": "db0ddccc-0b60-4603-dd77-f2eec70bc2b5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "X train: (4702, 128, 128, 3)\n",
            "Y train: (4702, 128, 128, 1)\n"
          ]
        }
      ],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from keras.models import Sequential, Model\n",
        "from tensorflow.keras.utils import plot_model\n",
        "from tensorflow.keras.optimizers import Adam, RMSprop\n",
        "import keras\n",
        "\n",
        "# load vgg model\n",
        "from keras.applications.vgg16 import VGG16, preprocess_input\n",
        "\n",
        "# SPLIT GREY DATAFRAME INTO 50/50 \n",
        "## WITH STRATIFICATION ON THE CLASSES COL\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    img_arr, mask_arr, \n",
        "    test_size=0.75, random_state=420\n",
        "    )\n",
        "## Scale the data\n",
        "scalar = MinMaxScaler()\n",
        "scalar.fit(X_train)\n",
        "#X_train = scalar.transform(X_train)\n",
        "#X_test = scalar.transform(X_test)\n",
        "X_train = unflatten(X_train, img_shape)[:,:,:,:3]\n",
        "#X_train = preprocess_input(X_train)\n",
        "X_test = unflatten(X_test, img_shape)[:,:,:,:3]\n",
        "#X_test = preprocess_input(X_test)\n",
        "y_train = unflatten(y_train, mask_shape)\n",
        "y_test = unflatten(y_test, mask_shape)\n",
        "\n",
        "input_shape = X_train.shape[1:]\n",
        "print('X train: {}'. format(X_train.shape))\n",
        "print('Y train: {}'. format(y_train.shape))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cCnsm8Jonjas"
      },
      "source": [
        "## Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "03LlyUsgnmgd"
      },
      "source": [
        "### Import VGG16"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jZ2AvxBSeQte"
      },
      "outputs": [],
      "source": [
        "\n",
        "# load the model\n",
        "vgg16 = VGG16(\n",
        "    include_top = False,\n",
        "    input_shape = rgb_shape,\n",
        "    weights='imagenet'\n",
        ")\n",
        "vgg16.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sm1a4kWzkKkY"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import \\\n",
        "  Conv2D, Conv2DTranspose, MaxPooling2D, \\\n",
        "  Dense, Activation, Dropout, \\\n",
        "  Flatten, Input, Concatenate, \\\n",
        "  BatchNormalization\n",
        "from tensorflow.keras.initializers import RandomNormal\n",
        "\n",
        "def conv_block(inputs,num_filters):\n",
        "  x = Conv2D(num_filters,3,padding='same')(inputs)\n",
        "  x = BatchNormalization()(x)\n",
        "  x = Activation('relu')(x)\n",
        "  x = Conv2D(num_filters,3,padding='same')(x)\n",
        "  x = BatchNormalization()(x)\n",
        "  x = Activation('relu')(x) \n",
        "  return x\n",
        "\n",
        "def define_decoder(inputs,skip_layer,num_filters):\n",
        "  init = RandomNormal(stddev=0.02)\n",
        "  x = Conv2DTranspose(\n",
        "        num_filters,(2,2),\n",
        "        strides=(2,2),\n",
        "        padding='same',\n",
        "        kernel_initializer=init\n",
        "      )(inputs)  \n",
        "  g = Concatenate()([x,skip_layer])\n",
        "  g = conv_block(g, num_filters)\n",
        "  return g\n",
        "\n",
        "def vgg16_unet(input_shape):\n",
        "  inputs = Input(shape=input_shape)\n",
        "  vgg16 = VGG16(include_top=False,weights='imagenet',input_tensor=inputs)  \n",
        "  # We will extract encoder layers based on their output shape from vgg16 model  \n",
        "  s1 = vgg16.get_layer('block1_conv2').output  \n",
        "  s2 = vgg16.get_layer('block2_conv2').output  \n",
        "  s3 = vgg16.get_layer('block3_conv3').output  \n",
        "  s4 = vgg16.get_layer('block4_conv3').output    # bottleneck/bridege layer from vgg16\n",
        "  b1 = vgg16.get_layer('block5_conv3').output #32\n",
        "  \n",
        "  # Decoder Block\n",
        "  d1 = define_decoder(b1,s4,512)\n",
        "  d2 = define_decoder(d1,s3,256)\n",
        "  d3 = define_decoder(d2,s2,128)\n",
        "  d4 = define_decoder(d3,s1,64)  #output layer\n",
        "  outputs = Conv2D(1,1, padding='same', activation='sigmoid')(d4)\n",
        "  model = Model(inputs,outputs)\n",
        "  \n",
        "  return model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "axniRW6hnwH0"
      },
      "source": [
        "### Import Metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hC7vG9Jel7_c"
      },
      "outputs": [],
      "source": [
        "from tensorflow.math import reduce_sum\n",
        "\n",
        "# Dice Loss \n",
        "## a measure of overlap between two samples\n",
        "## ranges from 0 to 1 \n",
        "## a Dice coefficient of 1 denotes perfect and complete overlap\n",
        "smooth = 1e-15\n",
        "def dice_coef(y_true,y_pred):\n",
        "  y_true = Flatten()(y_true)\n",
        "  y_pred = Flatten()(y_pred)\n",
        "  intersection = reduce_sum(y_true*y_pred)\n",
        "  return (2. * intersection + smooth) / (reduce_sum(y_true) + reduce_sum(y_pred))\n",
        "  \n",
        "def dice_loss(y_true,y_pred):\n",
        "  return 1.0 - dice_coef(y_true,y_pred)\n",
        "\n",
        "# Intersection-Over-Union \n",
        "## a common evaluation metric for semantic image segmentation\n",
        "## true_positive / (true_positive + false_positive + false_negative)\n",
        "def iou(y_true,y_pred):\n",
        "  def f(y_true,y_pred):\n",
        "    intersection = (y_true*y_pred).sum()\n",
        "    union = y_true.sum() + y_pred.sum() - intersection\n",
        "    x = (intersection + 1e-15) / (union + 1e-15)\n",
        "    x = x.astype(np.float32)\n",
        "    return x\n",
        "  return tf.numpy_function(f,[y_true,y_pred],tf.float32)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7R0VHWQHn0ds"
      },
      "source": [
        "### Compile Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R8xAbBFllfzX",
        "outputId": "168eef7b-163f-410b-e290-0560c840cf64"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(Adam, self).__init__(name, **kwargs)\n"
          ]
        }
      ],
      "source": [
        "from tensorflow.keras.metrics import Precision, Recall\n",
        "\n",
        "model = vgg16_unet(rgb_shape)\n",
        "model.compile(\n",
        "    loss=dice_loss,\n",
        "    optimizer=Adam(lr = .001),\n",
        "    metrics=[dice_coef, iou, Recall(), Precision()]\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bk0MF-man3ce"
      },
      "source": [
        "### Fit Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "DN69hjc3t9UC"
      },
      "outputs": [],
      "source": [
        "tf.config.run_functions_eagerly(True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FeaKYjD1nWC1",
        "outputId": "40b837b1-b1af-481a-d3ff-516abe782a6d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/data/ops/structured_function.py:265: UserWarning: Even though the `tf.config.experimental_run_functions_eagerly` option is set, this option does not apply to tf.data functions. To force eager execution of tf.data functions, please use `tf.data.experimental.enable_debug_mode()`.\n",
            "  \"Even though the `tf.config.experimental_run_functions_eagerly` \"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/25\n",
            "147/147 [==============================] - 105s 652ms/step - loss: 0.4033 - dice_coef: 0.5967 - iou: 0.4375 - recall: 0.7591 - precision: 0.4856 - val_loss: 0.3248 - val_dice_coef: 0.6752 - val_iou: 0.5114 - val_recall: 0.8827 - val_precision: 0.5744\n",
            "Epoch 2/25\n",
            "147/147 [==============================] - 85s 582ms/step - loss: 0.2305 - dice_coef: 0.7696 - iou: 0.6271 - recall: 0.7877 - precision: 0.7646 - val_loss: 0.3496 - val_dice_coef: 0.6504 - val_iou: 0.4844 - val_recall: 0.5092 - val_precision: 0.9220\n",
            "Epoch 3/25\n",
            "147/147 [==============================] - 85s 580ms/step - loss: 0.1896 - dice_coef: 0.8104 - iou: 0.6828 - recall: 0.8107 - precision: 0.8168 - val_loss: 0.1882 - val_dice_coef: 0.8118 - val_iou: 0.6846 - val_recall: 0.8201 - val_precision: 0.8118\n",
            "Epoch 4/25\n",
            "147/147 [==============================] - 85s 579ms/step - loss: 0.1735 - dice_coef: 0.8265 - iou: 0.7059 - recall: 0.8234 - precision: 0.8358 - val_loss: 0.2257 - val_dice_coef: 0.7743 - val_iou: 0.6336 - val_recall: 0.6720 - val_precision: 0.9182\n",
            "Epoch 5/25\n",
            "147/147 [==============================] - 85s 580ms/step - loss: 0.1539 - dice_coef: 0.8461 - iou: 0.7345 - recall: 0.8395 - precision: 0.8567 - val_loss: 0.1652 - val_dice_coef: 0.8348 - val_iou: 0.7175 - val_recall: 0.8400 - val_precision: 0.8331\n",
            "Epoch 6/25\n",
            "147/147 [==============================] - 85s 580ms/step - loss: 0.1424 - dice_coef: 0.8576 - iou: 0.7518 - recall: 0.8514 - precision: 0.8681 - val_loss: 0.1515 - val_dice_coef: 0.8485 - val_iou: 0.7382 - val_recall: 0.8153 - val_precision: 0.8880\n",
            "Epoch 7/25\n",
            "147/147 [==============================] - 84s 575ms/step - loss: 0.1358 - dice_coef: 0.8642 - iou: 0.7621 - recall: 0.8542 - precision: 0.8779 - val_loss: 0.1416 - val_dice_coef: 0.8584 - val_iou: 0.7531 - val_recall: 0.8372 - val_precision: 0.8831\n",
            "Epoch 8/25\n",
            "147/147 [==============================] - 84s 574ms/step - loss: 0.1287 - dice_coef: 0.8713 - iou: 0.7731 - recall: 0.8614 - precision: 0.8837 - val_loss: 0.1391 - val_dice_coef: 0.8609 - val_iou: 0.7570 - val_recall: 0.8374 - val_precision: 0.8883\n",
            "Epoch 9/25\n",
            "147/147 [==============================] - 84s 575ms/step - loss: 0.1245 - dice_coef: 0.8755 - iou: 0.7797 - recall: 0.8658 - precision: 0.8887 - val_loss: 0.1332 - val_dice_coef: 0.8668 - val_iou: 0.7661 - val_recall: 0.8375 - val_precision: 0.9005\n",
            "Epoch 10/25\n",
            "147/147 [==============================] - 84s 576ms/step - loss: 0.1152 - dice_coef: 0.8848 - iou: 0.7944 - recall: 0.8755 - precision: 0.8959 - val_loss: 0.1859 - val_dice_coef: 0.8141 - val_iou: 0.6884 - val_recall: 0.7160 - val_precision: 0.9462\n",
            "Epoch 11/25\n",
            "147/147 [==============================] - 84s 575ms/step - loss: 0.1121 - dice_coef: 0.8879 - iou: 0.7994 - recall: 0.8765 - precision: 0.9013 - val_loss: 0.1268 - val_dice_coef: 0.8732 - val_iou: 0.7760 - val_recall: 0.8772 - val_precision: 0.8714\n",
            "Epoch 12/25\n",
            "147/147 [==============================] - 84s 576ms/step - loss: 0.1086 - dice_coef: 0.8915 - iou: 0.8051 - recall: 0.8825 - precision: 0.9031 - val_loss: 0.1263 - val_dice_coef: 0.8737 - val_iou: 0.7767 - val_recall: 0.8450 - val_precision: 0.9061\n",
            "Epoch 13/25\n",
            "147/147 [==============================] - 84s 575ms/step - loss: 0.1038 - dice_coef: 0.8962 - iou: 0.8129 - recall: 0.8877 - precision: 0.9067 - val_loss: 0.1291 - val_dice_coef: 0.8709 - val_iou: 0.7724 - val_recall: 0.8340 - val_precision: 0.9131\n",
            "Epoch 14/25\n",
            "147/147 [==============================] - 84s 575ms/step - loss: 0.0994 - dice_coef: 0.9006 - iou: 0.8198 - recall: 0.8925 - precision: 0.9107 - val_loss: 0.1249 - val_dice_coef: 0.8751 - val_iou: 0.7790 - val_recall: 0.8403 - val_precision: 0.9145\n",
            "Epoch 15/25\n",
            "147/147 [==============================] - 85s 577ms/step - loss: 0.1013 - dice_coef: 0.8987 - iou: 0.8168 - recall: 0.8886 - precision: 0.9111 - val_loss: 0.1399 - val_dice_coef: 0.8601 - val_iou: 0.7559 - val_recall: 0.8913 - val_precision: 0.8328\n",
            "Epoch 16/25\n",
            "147/147 [==============================] - 85s 577ms/step - loss: 0.0946 - dice_coef: 0.9054 - iou: 0.8279 - recall: 0.8975 - precision: 0.9149 - val_loss: 0.1181 - val_dice_coef: 0.8819 - val_iou: 0.7898 - val_recall: 0.8731 - val_precision: 0.8926\n",
            "Epoch 17/25\n",
            "147/147 [==============================] - 85s 577ms/step - loss: 0.0918 - dice_coef: 0.9082 - iou: 0.8325 - recall: 0.9008 - precision: 0.9182 - val_loss: 0.1209 - val_dice_coef: 0.8791 - val_iou: 0.7854 - val_recall: 0.8834 - val_precision: 0.8767\n",
            "Epoch 18/25\n",
            "147/147 [==============================] - 84s 576ms/step - loss: 0.0864 - dice_coef: 0.9136 - iou: 0.8416 - recall: 0.9042 - precision: 0.9246 - val_loss: 0.1145 - val_dice_coef: 0.8855 - val_iou: 0.7955 - val_recall: 0.8750 - val_precision: 0.8980\n",
            "Epoch 19/25\n",
            "147/147 [==============================] - 84s 577ms/step - loss: 0.0895 - dice_coef: 0.9105 - iou: 0.8364 - recall: 0.9026 - precision: 0.9208 - val_loss: 0.1206 - val_dice_coef: 0.8794 - val_iou: 0.7858 - val_recall: 0.8672 - val_precision: 0.8939\n",
            "Epoch 20/25\n",
            "147/147 [==============================] - 85s 578ms/step - loss: 0.0923 - dice_coef: 0.9077 - iou: 0.8316 - recall: 0.9005 - precision: 0.9167 - val_loss: 0.1204 - val_dice_coef: 0.8796 - val_iou: 0.7862 - val_recall: 0.8555 - val_precision: 0.9067\n",
            "Epoch 21/25\n",
            "147/147 [==============================] - 84s 576ms/step - loss: 0.0814 - dice_coef: 0.9186 - iou: 0.8500 - recall: 0.9111 - precision: 0.9279 - val_loss: 0.1117 - val_dice_coef: 0.8883 - val_iou: 0.8001 - val_recall: 0.8751 - val_precision: 0.9033\n",
            "Epoch 22/25\n",
            "147/147 [==============================] - 85s 577ms/step - loss: 0.0777 - dice_coef: 0.9222 - iou: 0.8563 - recall: 0.9158 - precision: 0.9311 - val_loss: 0.1081 - val_dice_coef: 0.8919 - val_iou: 0.8060 - val_recall: 0.8705 - val_precision: 0.9160\n",
            "Epoch 23/25\n",
            "147/147 [==============================] - ETA: 0s - loss: 0.0745 - dice_coef: 0.9255 - iou: 0.8618 - recall: 0.9186 - precision: 0.9344"
          ]
        }
      ],
      "source": [
        "EPOCHS = 25\n",
        "#train_steps = len(X_train)//batch_size\n",
        "#test_steps = len(X_test)//batch_size\n",
        "\n",
        "history = model.fit(\n",
        "  X_train, tf.cast(y_train, tf.float32),\n",
        "  epochs=EPOCHS,\n",
        "  validation_data=(X_test, tf.cast(y_test, tf.float32))#,\n",
        "  #steps_per_epoch=train_steps,\n",
        "  #validation_steps=test_steps\n",
        "  )\n",
        "\n",
        "# convert the history.history dict to a pandas DataFrame:     "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oqH3CR16_ziY"
      },
      "outputs": [],
      "source": [
        "import datetime \n",
        "\n",
        "now = datetime.datetime.now()\n",
        "dt_string = now.strftime(\"%Y%m%d_%H%M\")\n",
        "\n",
        "model_path = os.path.join(DATA_DIR, 'models', 'vgg16_model_{}.keras'.format(dt_string))\n",
        "model.save(model_path)\n",
        "\n",
        "hist_df = pd.DataFrame(history.history)\n",
        "history_path = os.path.join(DATA_DIR, 'models', 'vgg16_history_{}.csv'.format(dt_string))\n",
        "with open(history_path, mode='w') as f:\n",
        "    hist_df.to_csv(f)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "reconstructed_model.history"
      ],
      "metadata": {
        "id": "xtLTTYqwYK4e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "puIM55yKbaFG"
      },
      "source": [
        "## Results"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Accuracy"
      ],
      "metadata": {
        "id": "xus-GXJ_ScDZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(model.history.history)"
      ],
      "metadata": {
        "id": "obfiIFlzSzPc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ylfhTu-TZuxr"
      },
      "outputs": [],
      "source": [
        "def plot_loss_acc(fitted_model, num_epoch = EPOCHS):\n",
        "  import matplotlib.pyplot as plt\n",
        "  fig, ax = plt.subplots(1,2, figsize=[18,6])\n",
        "  ax[0].plot(range(1, num_epoch+1), fitted_model.history['loss'], c='blue', label='Training loss')\n",
        "  ax[0].plot(range(1, num_epoch+1), fitted_model.history['val_loss'], c='red', label='Validation loss')\n",
        "  ax[0].legend()\n",
        "  ax[0].set_xlabel('epochs')\n",
        "\n",
        "  ax[1].plot(range(1, num_epoch+1), fitted_model.history['accuracy'], c='blue', label='Training accuracy')\n",
        "  ax[1].plot(range(1, num_epoch+1), fitted_model.history['val_accuracy'], c='red', label='Validation accuracy')\n",
        "  ax[1].legend()\n",
        "  ax[1].set_xlabel('epochs')\n",
        "\n",
        "plot_loss_acc(model, EPOCHS)\n",
        "#classification_accuracy = max(model.history['accuracy'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_X9n7dlD8EuU"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import random\n",
        "from skimage.io import imread\n",
        "import matplotlib\n",
        "from matplotlib import colors\n",
        "\n",
        "# counts array equal to focus value\n",
        "def count_focus(array, value=1):\n",
        "    return len(array[array==value])\n",
        "# get percent that the array is equal to value\n",
        "## used for filter masks\n",
        "def get_pct_arr(focus_array, value=1):\n",
        "    return np.array([count_focus(arr, value=value)/arr.size for arr in focus_array])\n",
        "\n",
        "smooth = 1e-15\n",
        "def dice_coef_arr(y_true,y_pred):\n",
        "  y_true = np.array(y_true).flatten()\n",
        "  y_pred = np.array(y_pred).flatten()\n",
        "  intersection = reduce_sum(y_true*y_pred)\n",
        "  return (2. * intersection + smooth) / (reduce_sum(y_true) + reduce_sum(y_pred))\n",
        "  \n",
        "def dice_loss_arr(y_true,y_pred):\n",
        "  return 1.0 - dice_coef(y_true,y_pred)\n",
        "\n",
        "def arr_to_binary(arr, cutoff=1.0, buffer=.01, good=1, bad=0):\n",
        "  arr[arr>=cutoff-buffer] = good\n",
        "  arr[arr<cutoff-buffer] = bad\n",
        "  return arr\n",
        "\n",
        "def get_masked_cmap(arr, thresh = 1.0, cmap='red'):\n",
        "  masked_array = np.ma.masked_where(arr < thresh, arr)\n",
        "  cmap = colors.ListedColormap([cmap])\n",
        "  return masked_array, cmap\n",
        "\n",
        "cols, rows = 4, 5\n",
        "axes = []\n",
        "[[axes.append((c,r)) for r in range(rows)] for c  in range(cols)]\n",
        "axis_count = len(axes)\n",
        "fig, ax = plt.subplots(rows, cols, figsize=[12, 16])\n",
        "\n",
        "for row, idx in enumerate(random.sample(range(len(X_test)), rows)):\n",
        "\n",
        "    focus_image, observed_mask = X_test[idx:idx+1,:,:,:], y_test[idx:idx+1,:,:,:]\n",
        "    \n",
        "    predicted_mask = model.predict(tf.cast(focus_image, tf.float32))\n",
        "\n",
        "    predicted_mask_thresh = arr_to_binary(predicted_mask.copy(), buffer=0.01)\n",
        "    dice_loss = dice_loss_arr(tf.cast(observed_mask, tf.float32), predicted_mask_thresh)\n",
        "    \n",
        "    pct_observed = get_pct_arr(observed_mask)[0]\n",
        "    pct_predicted = get_pct_arr(predicted_mask)[0]\n",
        "    pct_predicted_thresh = get_pct_arr(predicted_mask_thresh)[0]\n",
        "\n",
        "    ax[row, 0].imshow(focus_image[0])\n",
        "    ax[row, 0].set_title('RGB Image' + '\\n' + 'Dice Loss: {:.2f}'.format(dice_loss))\n",
        "\n",
        "    ax[row, 1].imshow(focus_image[0])\n",
        "    ax[row, 1].imshow(observed_mask.reshape(128,128), alpha=.5)\n",
        "    ax[row, 1].set_title('Right-of-Way Mask' + '\\n' + 'Observed Pct: {0:.0%}'.format(pct_observed))\n",
        "    \n",
        "\n",
        "    \n",
        "    ax[row, 2].imshow(focus_image[0])\n",
        "    ax[row, 2].imshow(predicted_mask.reshape(128,128), alpha=.5)\n",
        "\n",
        "    diff_mask = arr_to_binary(predicted_mask - observed_mask, buffer=0.01, bad=np.nan).reshape(128,128)\n",
        "    diff_mask, cmap = get_masked_cmap(diff_mask, cmap='green')\n",
        "    ax[row, 2].imshow(diff_mask, cmap=cmap)\n",
        "    diff_mask  = arr_to_binary(observed_mask - predicted_mask, buffer=0.01, bad=np.nan).reshape(128,128)\n",
        "    diff_mask, cmap = get_masked_cmap(diff_mask, cmap='red')\n",
        "    ax[row, 2].imshow(diff_mask, cmap=cmap)\n",
        "    #ax[row, 2].set_title('Pct Roads: {0:.1%}'.format(pct_roads) + '\\n' + 'Pct Diff: {0:.1%}'.format(pct_diff))\n",
        "    ax[row, 2].set_title('Predicted Mask' + '\\n' + 'Predicted Pct: {0:.0%}'.format(pct_predicted))\n",
        "    \n",
        "    \n",
        "\n",
        "    ax[row, 3].imshow(focus_image[0])\n",
        "    ax[row, 3].imshow(predicted_mask_thresh.reshape(128,128), alpha=.5)\n",
        "\n",
        "    diff_mask = arr_to_binary(predicted_mask_thresh - observed_mask, buffer=0.01, bad=np.nan).reshape(128,128)\n",
        "    diff_mask, cmap = get_masked_cmap(diff_mask, cmap='green')\n",
        "    ax[row, 3].imshow(diff_mask, cmap=cmap)\n",
        "    diff_mask  = arr_to_binary(observed_mask - predicted_mask_thresh, buffer=0.01, bad=np.nan).reshape(128,128)\n",
        "    diff_mask, cmap = get_masked_cmap(diff_mask, cmap='red')\n",
        "    ax[row, 3].imshow(diff_mask, cmap=cmap)\n",
        "    #ax[row, 3].imshow(predicted_mask_thresh.reshape(128,128), alpha=.5)\n",
        "    ax[row, 3].set_title('Predicted Mask (p>1)' + '\\n' + 'Predict Thresh Pct: {0:.0%}'.format(pct_predicted_thresh))\n",
        "\n",
        "    for col in range(cols):\n",
        "        ax[row, col].set_axis_off()\n",
        "fig.patch.set_facecolor('#FFFFFF')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AiPZnO-6xpdd"
      },
      "outputs": [],
      "source": [
        "arr = arr_to_binary(observed_mask - predicted_mask_thresh, buffer=0, bad=0).reshape(128,128)\n",
        "thresh=1\n",
        "np.ma.masked_where(arr < thresh, arr)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H3VuCV6pqf_l"
      },
      "outputs": [],
      "source": [
        "diff_mask[~np.isnan(diff_mask)]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mgHJwIkOYt__"
      },
      "outputs": [],
      "source": [
        "smooth = 1e-15\n",
        "def dice_coef(y_true,y_pred):\n",
        "  y_true = np.array(y_true).flatten()\n",
        "  y_pred = np.array(y_pred).flatten()\n",
        "  intersection = reduce_sum(y_true*y_pred)\n",
        "  return (2. * intersection + smooth) / (reduce_sum(y_true) + reduce_sum(y_pred))\n",
        "  \n",
        "def dice_loss(y_true,y_pred):\n",
        "  return 1.0 - dice_coef(y_true,y_pred)\n",
        "\n",
        "dice_loss(tf.cast(focus_image, tf.float32), predicted_image)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pcqUVaXocVOB"
      },
      "outputs": [],
      "source": [
        "X_test"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "background_execution": "on",
      "collapsed_sections": [
        "KDW58bu730gA",
        "XazlWhmB4OdZ"
      ],
      "machine_shape": "hm",
      "name": "FinalProject_model",
      "provenance": [],
      "authorship_tag": "ABX9TyPTj3By3sf4noLszxFqbF0x",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}